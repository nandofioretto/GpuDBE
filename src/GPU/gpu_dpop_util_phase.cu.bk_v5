// CUDA and CUBLAS functions
// CUDA runtime
#include <cuda_runtime.h>

// Utilities and system includes
#include <assert.h>
#include <helper_string.h>  // helper for shared functions common to CUDA Samples
#include <helper_functions.h>
#include <helper_cuda.h>

#include <vector>
#include <string>
#include <iostream>
#include <limits>       // std::numeric_limits
#include <cmath>       /* ceil */
#include <math_functions.h>
#include <cuda_profiler_api.h>

#include "GPU/gpu_dpop_util_phase.hh"
#include "GPU/gpu_globals.hh"
#include "GPU/gpu_data_allocator.hh"
#include "GPU/cuda_utils.hh"
#include "GPU/cuda_dpop_state.hh"

#include "Kernel/types.hh"

//#define VERBOSE

using namespace CUDA;

__global__ void compute_util_table_ver_0(int* utilTable,
		unsigned int block_shift,
		unsigned int nb_util_table_rows,  // after projection
		int nbGroups, // each group computes a row (nb_worlds)
		int aid, int d_size, int nb_var_sep,
		int nb_binary);

__global__ void compute_util_table_ver_1(int* utilTable, int** childUtilTable,
		unsigned int block_shift,
		unsigned int nb_util_table_rows,  // after projection
		//int nbGroups, // each group computes a row (nb_worlds)
		int aid, int d_size, int nb_var_sep,
		int nb_binary, int children_info_size);
//		bool is_root);


__global__ void compute_util_table_ver_1Root(int* utilTable, int** childUtilTable,
		unsigned int block_shift,
		unsigned int nb_util_table_rows,  // after projection
		//int nbGroups, // each group computes a row (nb_worlds)
		int aid, int d_size, int nb_var_sep,
		int nb_binary, int children_info_size);
//		bool is_root);

__global__ void compute_util_table_ver_2(int* devUtilTable,
		unsigned int blockShift,
		unsigned int utilTableSize,  // size after projection
		int aid, int domSize, int nbVarSep,
		int nbBinary);

__global__ void printArray(int** array, int id, unsigned int nRows, int nCols) {
	printf("Table nRwos=%d, nCols=%d\n", nRows, nCols);

	int limit = nRows > 50 ? 10 : nRows;
	for (int r = 0; r < limit; r++) {
		for (int c = 0; c < nCols; c++) {
			printf("%d ", array[id][r * nCols + c]);
		}
		printf("\n");
	}
}

__global__ void associateVector(int** array2d, int id, int* array) {
	array2d[id] = array;
}

// _version_ = 0  (tree-leaves)
//                uses directly projected table, and optimizes values for the 
//                domain of the current variable on GPU
//
// _version_ = 1  (if it can copy all children table in global memory)
//                uses directly projected table, and optimizes values for the 
//                domain of the current variable on GPU
//                - A thread operates in a single world (and all domain elements of this agent)
//                - A group  operates on a row of the UTIL table
//                - n groups (based on 256 threads)
//
// _version_ = 2  (when children table cannot fit= in global memory)
//                Computes only binary constraints on GPU and operates onto unprojected table. 
//                - A thread operates  all worlds, on a given combinaion of values of the separator set 
//                 (excluding thus current variable).
//                - Groups = Threads (256)
//                - Children are added on CPU
//                - Projection and Optimization is made on CPU
//            
// _version_ = 3  (when children table cannot fit in global memory) 
//                Computes only binary constraints on GPU and operates onto unprojected table. 
//                For a given combinaion of values of the separator set (excluding thus 
//                current variable), a thread manages all worlds and all values of the domain 
//                of the current variable. 
//                Children are added on CPU
//                Projection and Optimization is made on CPU
//
void GPU_DPOPutilPhase::compute_util_table(DPOPstate& dpop_state, int _version_, size_t& gpu_time_us) {

  int nAgents = dpop_state.get_nb_dcop_agents();

  // NOTE: Based on the version let the DPOP_state create a table of appropriate size.
  size_t nbUtilTableRows = dpop_state.getUtilTableRows(); // before projection
  size_t nbUtilTableRowsAfterProj = dpop_state.getUtilTableRowsAfterProj();
  
  // ----------------------------------------------------------------------- //
  // GPU Device Setup
  // ----------------------------------------------------------------------- //
  size_t nbThreads = 128;  // Number of Parallel Threads per SM
  size_t nbGroups; // groups of threads
  size_t nbBlocks;
  size_t devPitch = 512;
  if (_version_ == 0 || _version_ == 1) {
    nbGroups = nbThreads;   // One group for each row of the utility table
    nbBlocks =
      (nbUtilTableRowsAfterProj % nbGroups == 0) ?
      (nbUtilTableRowsAfterProj / nbGroups) :
      (nbUtilTableRowsAfterProj / nbGroups) + 1;
  } else if (_version_ == 2 || _version_ == 3) {
    nbGroups = nbThreads;
    //nbBlocks  = nbUtilTableRows;
    nbBlocks =
      (nbUtilTableRowsAfterProj % nbGroups == 0) ?
      (nbUtilTableRowsAfterProj / nbGroups) :
      (nbUtilTableRowsAfterProj / nbGroups) + 1;
  }

  
  // ----------------------------------------------------------------------- //
  // Shared memory
  // ----------------------------------------------------------------------- //
  int sharedMem = dpop_state.get_nb_binary_constraints() * 3 * sizeof(int); // constraintInfo
  // if( has_unary )
  //   shared_mem += nb_worlds * d_size * sizeof(util_t);
  
  if (_version_ == 0) {
    sharedMem += nbGroups * dpop_state.get_separator().size() * sizeof(int); // __sep_values
  } else if (_version_ == 1) {
    sharedMem = 0; // rm binary constraints
    sharedMem += nbThreads * dpop_state.get_dom_size() * sizeof(int);  // __util_vector
    sharedMem += dpop_state.get_separator().size()  * sizeof(int);
  } else if (_version_ == 2) {
    sharedMem += nbThreads * (dpop_state.get_separator().size() + 1) * sizeof(int);  // sepValues
    sharedMem += nbThreads /* dpop_state.get_nb_worlds() */ * sizeof(int); // worldsUtil
  }
  assert(sharedMem <= CUDA::info::shared_memory); // TOT_SHARED_MEM);
  // printf("[GPU] Agent %d Shared Memory required %zu bytes \n", dpop_state.get_agent_id(), sharedMem);
  
  // ----------------------------------------------------------------------- //
  // Global memory
  // ----------------------------------------------------------------------- //
  int **devChildUtilTable;
  int **hostChildUtilTableMirror; // the pointers to be copied on the device;

  int *devUtilTable;
  int *hostUtilTable = dpop_state.getUtilTablePtr();
  
  size_t cudaFreeMem = CUDAutils::get_nb_bytes_free_global_memory();
  size_t nBytesHost = 0;
  size_t nBytesDev = 0;
  size_t devUtilTableRows = 0;
  
  if (_version_ == 0) {
    nBytesHost = nbUtilTableRowsAfterProj * sizeof(int);
    devUtilTableRows = nbUtilTableRowsAfterProj;
  } else if (_version_ == 1) {
    nBytesHost = nbUtilTableRowsAfterProj * sizeof(int);
    devUtilTableRows = nbUtilTableRowsAfterProj;
    
    // Add up the aggregated children Table memory and Remove it from CudaFreeMem
    size_t childrenMem = 0;
    std::vector<int> childrenId = dpop_state.getChildrenId();
    for (int i = 0; i < childrenId.size(); i++) {
      childrenMem += dpop_state.getChildTableRows(childrenId[i]) * sizeof(int);
      std::cout << "[GPU] Chid " << childrenId[i] << " required Mem: "
		<< childrenMem / (1024 * 1024) << " MB\n";
    }
    cudaFreeMem -= childrenMem;
  } else if (_version_ == 2 || _version_ == 3) {
    nBytesHost = nbUtilTableRows * sizeof(int);
    devUtilTableRows = nbUtilTableRows;
  }
  nBytesDev = nBytesHost;
  
  // We fit on Device whatever we can
  if (nBytesDev >= cudaFreeMem) {
    devUtilTableRows = cudaFreeMem / sizeof(int);
    // ensure it's a multiple of d:
    int rem = devUtilTableRows % dpop_state.get_dom_size();
    if (rem != 0)
      devUtilTableRows -= rem;
    
    nBytesDev = devUtilTableRows * sizeof(int);
  }
  
  if (_version_ == 0 || _version_ == 1)
    printf(
	   "[GPU] Agent %d Util Table Memory Needed: %zu MB, [%d] free memory %zu MB \n",
	   dpop_state.get_agent_id(), nBytesHost / (1024 * 1024),
	   nbUtilTableRowsAfterProj, cudaFreeMem / (1024 * 1024));
  else if (_version_ == 2 || _version_ == 3)
    printf(
	   "[GPU] Agent %d Util Table Memory Needed: %zu MB, [%d] free memory %zu MB \n",
	   dpop_state.get_agent_id(), nBytesHost / (1024 * 1024),
	   nbUtilTableRows, cudaFreeMem / (1024 * 1024));
  
  //-----------------------------------------------
  // Initialize Streams and Events
  //-----------------------------------------------
  std::vector<int> childrenId = dpop_state.getChildrenId();
  //const int nStreamsCh = childrenId.size();
  //cudaStream_t streamCh[nStreamsCh];
  cudaEvent_t startEvent, stopEvent;
  float ms, tot_ms = 0;
  
  checkCudaErrors( cudaEventCreate(&startEvent) );
  checkCudaErrors( cudaEventCreate(&stopEvent) );
//  for (int i = 0; i < nStreamsCh; ++i)
//    checkCudaErrors( cudaStreamCreate(&streamCh[i]) );

  //-----------------------------------------------
  // Copy Children Tables into Global Mem
  //-----------------------------------------------
  checkCudaErrors( cudaEventRecord(startEvent,0) );
  checkCudaErrors(cudaMalloc(&devUtilTable, nBytesDev));  

  if (_version_ == 1) {
    checkCudaErrors(cudaMalloc(&devChildUtilTable, nAgents * sizeof(int*)));    
    hostChildUtilTableMirror = new int*[childrenId.size()];
      
    for (int i = 0; i < childrenId.size(); i++) 
    {
      int chId = childrenId[i];
      unsigned int sizeChTable = dpop_state.getChildTableRows(chId) * sizeof(int);
      int* dev_tmp;
      checkCudaErrors(cudaMalloc(&dev_tmp, sizeChTable));
      checkCudaErrors(cudaMemcpyAsync(dev_tmp, dpop_state.getChildTablePtr(chId),
				       sizeChTable, cudaMemcpyHostToDevice));
		//		       streamCh[i]));
      associateVector<<<1,1,0/*,streamCh[i]*/>>> (devChildUtilTable, chId, dev_tmp);
      hostChildUtilTableMirror[i] = dev_tmp;
    }
  }
  checkCudaErrors( cudaEventRecord(stopEvent, 0) );
  checkCudaErrors( cudaEventSynchronize(stopEvent) );
  checkCudaErrors( cudaEventElapsedTime(&ms, startEvent, stopEvent) );
  printf("Time for asynchronous transfer and execute (ms): %f\n", ms);

    

  //-----------------------------------------------
  // Process UTIL Table
  //-----------------------------------------------
  //// note: MAX_DIM_GRID = TOT_GLOBAL_MEM-1
  //// Thus, we use as block shift exclusively the rows that have been computed so far (GLOBAL MEM)
  size_t cudaAggrBlocks = 0;
  size_t cudaAggrTableRows = 0;
  size_t cudaTableRowsLeft = 0;
  size_t nbTableRowsToCompute = 0;
  if (_version_ == 0 || _version_ == 1)
    nbTableRowsToCompute = nbUtilTableRowsAfterProj;
  else if (_version_ == 2 || _version_ == 3)
    nbTableRowsToCompute = nbUtilTableRows;


  //-----------------------------------------------
  // Initialize Streams and Events
  //-----------------------------------------------
  const int maxStreamSize = 10000000;
  const int nStreams = (devUtilTableRows / maxStreamSize) + 1;
  cudaStream_t stream[nStreams];
  for (int i = 0; i < nStreams; ++i)
    checkCudaErrors(cudaStreamCreate(&stream[i]));

  checkCudaErrors(cudaEventRecord(startEvent, 0));
  do 
  {
    cudaTableRowsLeft = (nbTableRowsToCompute - cudaAggrTableRows);
    
    if (devUtilTableRows > cudaTableRowsLeft)
      devUtilTableRows = cudaTableRowsLeft;
        
    // 48 / 10 -> 5 [10, 10, 10, 10, 8]
    // ----------------------------------------------------------------------- //
    // EXECUTE KERNELs
    // ----------------------------------------------------------------------- //
    size_t nbBlocksCompleted = 0;
    for (int i=0; i<nStreams; i++) 
    {
      int streamUtilRows = i < (nStreams-1) ? maxStreamSize : devUtilTableRows % maxStreamSize;
      // Update nBlocks with the current devUtilTableRows info:
      nbBlocks = (streamUtilRows % nbGroups == 0) ? 
			       (streamUtilRows / nbGroups) 
			       : (streamUtilRows / nbGroups) + 1;
   
      nBytesDev = streamUtilRows * sizeof(int);
      printf("[GPU] Device Util Table size: [%zu] (MB=%zu)\n",
	     streamUtilRows, nBytesDev / (1024 * 1024));
      printf("[GPU][%d] Kernel: nbBlocks=%zu nbStreams=%zu nbThreads=%zu\n", i, nbBlocks, nStreams, nbThreads);

      size_t runningNbBlocks = nbBlocks; // > MAX_DIM_GRID ? MAX_DIM_GRID : nbBlocks;

      if (_version_ == 0) {
	// std::cout << "Running version 0\n";
	compute_util_table_ver_0<<< runningNbBlocks, nbThreads, sharedMem >>>
	  (devUtilTable,
	   cudaAggrTableRows + nbBlocksCompleted,
	   streamUtilRows,
	   nbGroups,
	   dpop_state.get_agent_id(),
	   dpop_state.get_dom_size(),
	   dpop_state.get_separator().size(),
	   dpop_state.get_nb_binary_constraints());
      }
      else if (_version_ == 1) {
	// std::cout << "Running version 1\n";
	// std::cout << "ch info size: " << dpop_state.get_children_info_size() << "\n";
	// CUDAutils::dump_agent_info( dpop_state.get_agent_id() );       
	if(dpop_state.is_root()) {
	  compute_util_table_ver_1Root<<< runningNbBlocks, nbThreads, sharedMem, stream[i] >>>
	    (devUtilTable,
	     devChildUtilTable,
	     cudaAggrTableRows + nbBlocksCompleted,
	     streamUtilRows,
	     dpop_state.get_agent_id(),
	     dpop_state.get_dom_size(),
	     dpop_state.get_separator().size(),
	     dpop_state.get_nb_binary_constraints(),
	     dpop_state.get_children_info_size());
	} else {
	  compute_util_table_ver_1<<< runningNbBlocks, nbThreads, sharedMem, stream[i] >>>
	    (devUtilTable,
	     devChildUtilTable,
	     cudaAggrTableRows + nbBlocksCompleted,
	     streamUtilRows,
	     dpop_state.get_agent_id(),
	     dpop_state.get_dom_size(),
	     dpop_state.get_separator().size(),
	     dpop_state.get_nb_binary_constraints(),
	     dpop_state.get_children_info_size());
	}
      }
      else if (_version_ == 2) {
	// std::cout << "Running version 2\n";
	compute_util_table_ver_2<<< runningNbBlocks, nbThreads, sharedMem >>>
	  (devUtilTable,
	   cudaAggrBlocks + nbBlocksCompleted,
	   streamUtilRows,
	   dpop_state.get_agent_id(),
	   dpop_state.get_dom_size(),
	   dpop_state.get_separator().size() + 1,
	   dpop_state.get_nb_binary_constraints());
      }
      //checkCudaErrors(cudaDeviceSynchronize());      
      // tot_ms += ms;
      nbBlocksCompleted += runningNbBlocks;

      // Copy Memory Back: Device --> Host
      checkCudaErrors(cudaMemcpyAsync(&hostUtilTable[cudaAggrTableRows], devUtilTable,
				      nBytesDev, cudaMemcpyDeviceToHost, stream[i]));
      cudaAggrBlocks += nbBlocks;
      cudaAggrTableRows += streamUtilRows;
    }
    
   checkCudaErrors(cudaDeviceSynchronize());
    
    
  } while (cudaAggrTableRows < nbTableRowsToCompute);
  // If _version_ 2 or 3 we still need to integrate the children table (on Host)
  
  checkCudaErrors(cudaEventRecord(stopEvent, 0));
  checkCudaErrors(cudaEventSynchronize(stopEvent));
  checkCudaErrors(cudaEventElapsedTime(&ms, startEvent, stopEvent));
  printf("[GPU] gpu-time elapsed %f ms\n", ms);
  gpu_time_us = ms * 1000;
  

  // ------------------------------------------------
  // cleanup
  // ------------------------------------------------
  checkCudaErrors( cudaEventDestroy(startEvent) );
  checkCudaErrors( cudaEventDestroy(stopEvent) );
  for (int i = 0; i < nStreams; ++i)
    checkCudaErrors( cudaStreamDestroy(stream[i]) );

  // TODO: This should not be done in Bucket Elimination (COP)
  checkCudaErrors(cudaFree(devUtilTable));
  if (_version_ == 1) {
    for (int i = 0; i < dpop_state.getChildrenId().size(); i++) {
      checkCudaErrors(cudaFree(hostChildUtilTableMirror[i]));
    }
    checkCudaErrors(cudaFree(devChildUtilTable));
    delete[] hostChildUtilTableMirror;
  }
  
}



/////////////////////////////////////////////////////////////////////////////////////////////////////////////
__device__ __forceinline__
unsigned int lcuda_encode(int* t, int t_size, int d) {
  int _d = d;
  unsigned int ofs = t[--t_size];
  #pragma unroll
  while (t_size > 0) {
    ofs += t[--t_size] * _d;
    _d *= d;
  }
  return ofs;
}

__device__ __forceinline__
unsigned int lcuda_fencode_next(int code, int t_size, int pos, int d) {
  return code + pow(d, t_size-pos-1); 
}

__device__ __forceinline__
void lcuda_decode(unsigned int code, int* t, int t_size, int d) {
  #pragma unroll
  for (int i = t_size - 1; i >= 0; i--) {
    t[i] = code % d;
    code /= d;
  }
}

__device__ __forceinline__
int lcuda_decode(const unsigned int& code, const int& pos, const int* dPow, const int& d) {
  return (code / dPow[pos]) % d;
}

__device__ __forceinline__
void lcuda_get_dPow(int* dPow, int dPow_size, int d) {
  if (dPow_size == 0) return;
  dPow[dPow_size-1] = 1;
  #pragma unroll
  for (int i = dPow_size-2; i >= 0; i--) {
    dPow[i] = dPow[i+1] * d;
  }
}

////////////////////////////////////////////////////////////////////////////////
// NOTE: 
// Thread's private array definitely is stored at local memory space, in the DRAM off-the-chip, 
// and maybe cached in memory hierarchy. Generally, non-array variable are considered as virtual 
// registers in PTX and the number of registers in PTX are unlimited. However, obviously all these 
// virtual registers are not mapped to physical registers. A PTX postprocessor spills some registers 
// to local space according to the micro-architecture flags specified for NVCC, and optimizes the register usage.
////////////////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////
// C U D A   K E R N E L S  ( UTIL Table computation )
////////////////////////////////////////////////////////////////////////////////////////
////////////////////////////////////////////////////////////////////////////////////////

////////////////////////////////////////////////////////////////////////////////////////
// V E R S I O N   0   (Leaves)
////////////////////////////////////////////////////////////////////////////////////////
__global__ void compute_util_table_ver_0(int* utilTable,
		unsigned int block_shift,
		unsigned int devUtilTableRows,  // after projection
		int nbGroups, // each group computes a row (nb_worlds)
		int aid, int d_size, int nb_var_sep,
		int nb_binary) {
	// ---------------------------------------------------------------------- //
	// Registers
	// ---------------------------------------------------------------------- //
	int groupIdx = threadIdx.x; // One thread = one row of utilTable
	if (groupIdx >= nbGroups) return;

	unsigned int utilTableRow = (blockIdx.x * nbGroups) + groupIdx;
	unsigned int utilTableCode = block_shift + utilTableRow;

	if (utilTableRow >= devUtilTableRows)
		return;

	// printf("Thread %zu on world %d (GroupID %d) - utilRow: %d \n", threadIdx.x, wid, groupIdx, (int)utilTableRow);

	int util_di = 0;
	int util = 0;
	int _i = 0, _j = 0, _di = 0, _id = 0, _x1 = 0, _x2 = 0;
	int _scope_x1, _scope_x2;
	int _ch_sep_size = 0, _c_code;

	// ---------------------------------------------------------------------- //
	// Shared Memory Allocation
	// ---------------------------------------------------------------------- //
	extern __shared__ int __smem[];
	int* __constraint = __smem; //gdev_DPOP_Agents[aid].binary_con;

	if (threadIdx.x == 0) {
			if (nb_binary > 0)
				memcpy(__constraint, gdev_DPOP_Agents[aid].binary_con,
						nb_binary * 3 * sizeof(int));
	}

	int* __sep_values = &__smem[nb_binary * 3 + (groupIdx * nb_var_sep)];
	__syncthreads();

	lcuda_decode(utilTableCode, __sep_values, nb_var_sep, d_size);
	__syncthreads();

	// ---------------------------------------------------------------------- //
	// Compute Util Table Entry Value
	util = UNSAT;
	for (_di = 0; _di < d_size; _di++) // Reticulate across al domain elements
	{
		util_di = 0;

		//--------------------------------------------------------------------- //
		// Binary constraints
		//--------------------------------------------------------------------- //
		#pragma unroll
		for (_i = 0; _i < nb_binary; _i++) {
			_id = __constraint[3 * _i];
			_x1 = __constraint[3 * _i + 1];
			_x2 = __constraint[3 * _i + 2];
			_scope_x1 = _x1 == -1 ? _di : __sep_values[_x1];
			_scope_x2 = _x2 == -1 ? _di : __sep_values[_x2];

			_j = gdev_Constraints[_id].utils[_scope_x1 * d_size + _scope_x2]; // (global mem)

			// printf("Thread %d on world %d - constraint c_%d [%d %d] = %d\n",
			// 	     threadIdx.x, wid, _id, _scope_x1, _scope_x2, _j);

			if (_j == UNSAT) {
				util_di = UNSAT;
				break;
			}
			util_di += _j;
		}

		if (util == UNSAT || util_di > util) {
			util = util_di;
		}
	}

	utilTable[utilTableRow] = util; // (global mem)
}


////////////////////////////////////////////////////////////////////////////////////////
// V E R S I O N   1   (Small (<2G) Util Tables (Children Table can be copied to Global Mem)
////////////////////////////////////////////////////////////////////////////////////////
__global__ void compute_util_table_ver_1(int* utilTable,
					 int** childIdToUtilTablePtr,
					 // childUtilTableRows ?
					 unsigned int block_shift, unsigned int devUtilTableRows,
					 //int nbGroups, // each group computes a row (nb_worlds)
					 int aid, int d_size, int nb_var_sep,
					 //int max_ch_sep_size,
					 int nb_binary, int children_info_size) {
//					 bool is_root) {
  // ---------------------------------------------------------------------- //
  // Registers
  // ---------------------------------------------------------------------- //
  unsigned int utilTableRow = (blockIdx.x * blockDim.x) + threadIdx.x;
  unsigned int utilTableCode = block_shift + utilTableRow;
  
  if (utilTableRow >= devUtilTableRows)
    return;
  
  int _i = 0, _j = 0, _k = 0, _di = 0, _id = 0, _x1 = 0, _x2 = 0;
  int _scope_x1, _scope_x2;
  int _ch_sep_size = 0;
  unsigned int _d_power = 1;

  // ---------------------------------------------------------------------- //
  // Shared Memory Allocation
  // ---------------------------------------------------------------------- //
  extern __shared__ int __smem[];  
  _j = 0;
  int* __util_vector = &__smem[_j + (threadIdx.x * d_size)];
  _j += blockDim.x * d_size;
  int* __dPow = &__smem[_j];
    lcuda_get_dPow(__dPow, nb_var_sep, d_size);
  __syncthreads();
    
  // ---------------------------------------------------------------------- //
  // Compute Util Table Entry Value
  // ---------------------------------------------------------------------- //
  for (_i = 0; _i < d_size; _i++) __util_vector[_i] = 0;


  // ---------------------------------------------------------------------- //
  // Global Memory Links
  // ---------------------------------------------------------------------- //
  //int* g_constraint = gdev_DPOP_Agents[aid].binary_con;
  //int* g_children_info = gdev_DPOP_Agents[aid].children_info;
  int* g_Container;  // used for [g_constraint, g_children_info)
  int* g_UtilTable;  // used for [g_con_utils, g_child_utils]

  g_Container = gdev_DPOP_Agents[aid].binary_con;
  //--------------------------------------------------------------------- //
  // Binary constraints
  //--------------------------------------------------------------------- //
  for (_i = 0; _i < nb_binary; _i++) {
    _id = g_Container[3 * _i];      // O(1)
    _x1 = g_Container[3 * _i + 1];  // .
    _x2 = g_Container[3 * _i + 2];  // .

    //int* g_con_utils = gdev_Constraints[_id].utils;  // O(T)
	g_UtilTable = gdev_Constraints[_id].utils;  // O(T)

    #pragma unroll
    for (_di = 0; _di < d_size; _di++) {
      _scope_x1 = _x1 == -1 ? _di : ((utilTableCode / __dPow[_x1]) % d_size);  // ~ O(1)
      _scope_x2 = _x2 == -1 ? _di : ((utilTableCode / __dPow[_x2]) % d_size);  // ~ O(1)
      
      // Two solutions to speed up this step:
      // 1. Align: if x1==-1 -> need transpose, so that threads can copy more data.
      // 2. Use d_size threads (BEST solution) Each thread read one cell and adds 
	__util_vector[_di] += g_UtilTable[_scope_x1 * d_size + _scope_x2]; // O(T) (global mem)
    }
  }

  g_Container = gdev_DPOP_Agents[aid].children_info;
  //--------------------------------------------------------------------- //
  // Messages from Children
  //--------------------------------------------------------------------- //
  _i = 0;
  //#pragma unroll
  while (_i < children_info_size) {
    _id = g_Container[_i++]; // O(1)
    _ch_sep_size = g_Container[_i++];  // O(1);
    
    if (_ch_sep_size <= 0)
      continue;

    _d_power = 1;
    _k = -1;
    //int* g_child_utils = childIdToUtilTablePtr[_id]; // O(1)
    g_UtilTable = childIdToUtilTablePtr[_id]; // O(1)

    // Do it in decreasing order to better compute the domain power 
    _i = _i + _ch_sep_size - 1;
    _x2 = 0;
    // MAYBE CAN REM. J
    #pragma unroll
    for (_j = _ch_sep_size-1; _j >= 0; _j--) {
      _x1 = g_Container[_i--];  // O(1)
      _k = (_x1 == -1) ? _j : _k;        // index of sep_values (-1 if current agent)

      // Two solutions to speed it up: 
      // * 1. Save it once in shared - reuse it for each _di
      // 2. Use d_size threads.
      _x2 += (_x1 == -1) ? 0 :  ((utilTableCode / __dPow[_x1]) % d_size) * _d_power; // ~ O(1)
      _d_power *= d_size;
    }
    _i += _ch_sep_size + 1;
    _d_power = 1;
    
    #pragma unroll
    for (_j = 0; _j < _ch_sep_size-_k-1; _j++) _d_power *= d_size;
    
    #pragma unroll
    for (_di = 0; _di < d_size; _di++) {
      __util_vector[_di] += g_UtilTable[_x2]; // O(T)
      _x2 += _d_power;
    }
  }

  // get best util:
  _x2/*UTIL*/ = UNSAT;
  #pragma unroll
  for (_di = 0; _di < d_size; _di++)
	  _x2/*UTIL*/ = fmax((double)_x2/*UTIL*/, (double)__util_vector[_di]);

  utilTable[utilTableRow] = _x2/*UTIL*/; // (global mem)
  
  if (aid == 9) {
    for (_di = 0; _di < d_size; _di++)
      _x1/*best_di*/ = _x2/*UTIL*/ == __util_vector[_di] ? _di : _x1/*best_di*/;
    gdev_DPOP_Agents[aid].best_value = _x1/*best_di*/;
    gdev_DPOP_Agents[aid].best_util = _x2/*UTIL*/;
  }
  
}


////////////////////////////////////////////////////////////////////////////////////////
// V E R S I O N   1   (Small (<2G) Util Tables (Children Table can be copied to Global Mem)
////////////////////////////////////////////////////////////////////////////////////////
__global__ void compute_util_table_ver_1Root(int* utilTable,
					 int** childIdToUtilTablePtr,
					 // childUtilTableRows ?
					 unsigned int block_shift, unsigned int devUtilTableRows,
					 //int nbGroups, // each group computes a row (nb_worlds)
					 int aid, int d_size, int nb_var_sep,
					 //int max_ch_sep_size,
					 int nb_binary, int children_info_size) {
//					 bool is_root) {
  // ---------------------------------------------------------------------- //
  // Registers
  // ---------------------------------------------------------------------- //
  unsigned int utilTableRow = (blockIdx.x * blockDim.x) + threadIdx.x;
  unsigned int utilTableCode = block_shift + utilTableRow;

  if (utilTableRow >= devUtilTableRows)
    return;

  int _i = 0, _j = 0, _k = 0, _di = 0, _id = 0, _x1 = 0, _x2 = 0;
  int _scope_x1, _scope_x2;
  int _ch_sep_size = 0;
  unsigned int _d_power = 1;

  // ---------------------------------------------------------------------- //
  // Shared Memory Allocation
  // ---------------------------------------------------------------------- //
  extern __shared__ int __smem[];
  _j = 0;
  int* __util_vector = &__smem[_j + (threadIdx.x * d_size)];
  _j += blockDim.x * d_size;
  int* __dPow = &__smem[_j];
    lcuda_get_dPow(__dPow, nb_var_sep, d_size);
  __syncthreads();

  // ---------------------------------------------------------------------- //
  // Compute Util Table Entry Value
  // ---------------------------------------------------------------------- //
  for (_i = 0; _i < d_size; _i++) __util_vector[_i] = 0;


  // ---------------------------------------------------------------------- //
  // Global Memory Links
  // ---------------------------------------------------------------------- //
  //int* g_constraint = gdev_DPOP_Agents[aid].binary_con;
  //int* g_children_info = gdev_DPOP_Agents[aid].children_info;
  int* g_Container;  // used for [g_constraint, g_children_info)
  int* g_UtilTable;  // used for [g_con_utils, g_child_utils]

  g_Container = gdev_DPOP_Agents[aid].binary_con;
  //--------------------------------------------------------------------- //
  // Binary constraints
  //--------------------------------------------------------------------- //
  for (_i = 0; _i < nb_binary; _i++) {
    _id = g_Container[3 * _i];      // O(1)
    _x1 = g_Container[3 * _i + 1];  // .
    _x2 = g_Container[3 * _i + 2];  // .

    //int* g_con_utils = gdev_Constraints[_id].utils;  // O(T)
	g_UtilTable = gdev_Constraints[_id].utils;  // O(T)

    #pragma unroll
    for (_di = 0; _di < d_size; _di++) {
      _scope_x1 = _x1 == -1 ? _di : ((utilTableCode / __dPow[_x1]) % d_size);  // ~ O(1)
      _scope_x2 = _x2 == -1 ? _di : ((utilTableCode / __dPow[_x2]) % d_size);  // ~ O(1)

      // Two solutions to speed up this step:
      // 1. Align: if x1==-1 -> need transpose, so that threads can copy more data.
      // 2. Use d_size threads (BEST solution) Each thread read one cell and adds
	__util_vector[_di] += g_UtilTable[_scope_x1 * d_size + _scope_x2]; // O(T) (global mem)
    }
  }

  g_Container = gdev_DPOP_Agents[aid].children_info;
  //--------------------------------------------------------------------- //
  // Messages from Children
  //--------------------------------------------------------------------- //
  _i = 0;
  //#pragma unroll
  while (_i < children_info_size) {
    _id = g_Container[_i++]; // O(1)
    _ch_sep_size = g_Container[_i++];  // O(1);

    if (_ch_sep_size <= 0)
      continue;

    _d_power = 1;
    _k = -1;
    //int* g_child_utils = childIdToUtilTablePtr[_id]; // O(1)
    g_UtilTable = childIdToUtilTablePtr[_id]; // O(1)

    // Do it in decreasing order to better compute the domain power
    _i = _i + _ch_sep_size - 1;
    _x2 = 0;
    #pragma unroll
    for (_j = _ch_sep_size-1; _j >= 0; _j--) {
      _x1 = g_Container[_i--];  // O(1)
      _k = (_x1 == -1) ? _j : _k;        // index of sep_values (-1 if current agent)

      // Two solutions to speed it up:
      // * 1. Save it once in shared - reuse it for each _di
      // 2. Use d_size threads.
      _x2 += (_x1 == -1) ? 0 :  ((utilTableCode / __dPow[_x1]) % d_size) * _d_power; // ~ O(1)
      _d_power *= d_size;
    }
    _i += _ch_sep_size + 1;
    _d_power = 1;

    #pragma unroll
    for (_j = 0; _j < _ch_sep_size-_k-1; _j++) _d_power *= d_size;

    #pragma unroll
    for (_di = 0; _di < d_size; _di++) {
      __util_vector[_di] += g_UtilTable[_x2]; // O(T)
      _x2 += _d_power;
    }
  }

  // get best util:
  _x2/*UTIL*/ = UNSAT;
  #pragma unroll
  for (_di = 0; _di < d_size; _di++)
	  _x2/*UTIL*/ = fmax((double)_x2/*UTIL*/, (double)__util_vector[_di]);

  utilTable[utilTableRow] = _x2/*UTIL*/; // (global mem)

  for (_di = 0; _di < d_size; _di++)
    _x1/*best_di*/ = _x2/*UTIL*/ == __util_vector[_di] ? _di : _x1/*best_di*/;
  gdev_DPOP_Agents[aid].best_value = _x1/*best_di*/;
  gdev_DPOP_Agents[aid].best_util = _x2/*UTIL*/;
}

////////////////////////////////////////////////////////////////////////////////////////
// V E R S I O N   2   (Very Large (>2GB) Util Tables)
////////////////////////////////////////////////////////////////////////////////////////
// Deals with a non-projected UTIL table. Each tread computes one row of the table
// (all worlds associated to it).
// One group computes 256 rows.
__global__ void compute_util_table_ver_2(int* devUtilTable,
		unsigned int blockShift,
		unsigned int devUtilTableRows,  // size after projection
		int aid, int domSize, int nbVarSep,
		int nbBinary) {
	// ---------------------------------------------------------------------- //
	// Registers
	// ---------------------------------------------------------------------- //
	unsigned int utilTableRow = threadIdx.x + blockIdx.x * blockDim.x; // Table Current In Memory
	unsigned int utilTableCode = blockShift + utilTableRow; // Global Table

	// Cannot compute a row that does not exists.
	if (utilTableRow >= devUtilTableRows)
		return;

	int _util = 0, __worldsUtil, _i = 0, _id = 0, _x1 = 0, _x2 = 0, _scope_x1 = 0, _scope_x2 = 0;
	// ---------------------------------------------------------------------- //
	// Shared Memory Allocation
	// ---------------------------------------------------------------------- //
	extern __shared__ int __smem[];
	int* __constraint = __smem;
	_i = nbBinary * 3;
	int* __sepValues = &__smem[_i + (threadIdx.x * nbVarSep)];
	_i += blockDim.x * nbVarSep;
	__syncthreads();

	if (nbBinary > 0 && threadIdx.x == 0) {
		memcpy(__constraint, gdev_DPOP_Agents[aid].binary_con,
				(nbBinary * 3 * sizeof(int)));
	}

	lcuda_decode(utilTableCode, __sepValues, nbVarSep, domSize);
	__syncthreads();

	// ---------------------------------------------------------------------- //
	// Compute Util Table Entry Value
	_util = -1;
	__worldsUtil = 0;
	//--------------------------------------------------------------------- //
	// Binary constraints
	//--------------------------------------------------------------------- //
	#pragma unroll
	for (_i = 0; _i < nbBinary; _i++) {
		_id = __constraint[3 * _i];
		_x1 = __constraint[3 * _i + 1];
		_x2 = __constraint[3 * _i + 2];
		// This variable domain are stored in the last position of the UtilTable before projection
		_scope_x1 =
				_x1 == -1 ? __sepValues[nbVarSep - 1] : __sepValues[_x1];
		_scope_x2 =
				_x2 == -1 ? __sepValues[nbVarSep - 1] : __sepValues[_x2];

		_util =
				gdev_Constraints[_id].utils[_scope_x1 * domSize + _scope_x2]; // (global mem)
			// printf("Thread %d on world %d -> row: %d constraint c_%d [%d %d] = %d\n",
			//  	     threadIdx.x, _wid, utilTableCode, _id, _scope_x1, _scope_x2, _util);

		if (_util == UNSAT) {
			__worldsUtil = UNSAT;
			break;
		}
		__worldsUtil += _util;
	}

	devUtilTable[utilTableRow] = __worldsUtil; // (global mem)

	// delete[] __worldsUtil;
}

// @Deprecated
__global__ void project_util_table(int *devUtilTable,
		unsigned int nbUtilTableRows, int domSize) {

	// Thread 0 -> take first (domSize rows) and all worlds
	unsigned int _start_table_row = threadIdx.x * domSize + blockIdx.x * blockDim.x * domSize;
	unsigned int _end_table_row = _start_table_row + domSize - 1;
	if (_end_table_row > nbUtilTableRows)
		return; // This should never happen
	// unsigned int _table_row_after_proj = threadIdx.x + blockIdx.x * blockDim.x;

	int _util_di, _best_util, _w, _d;

	_best_util = UNSAT;
	_util_di = UNSAT;

	#pragma unroll
	for (_d = 0; _d < domSize; _d++) {
		_util_di = devUtilTable[_start_table_row + _d];
		if (_util_di != UNSAT && _util_di > _best_util)
			_best_util = _util_di;
	}
	devUtilTable[_start_table_row] = _best_util;
}

